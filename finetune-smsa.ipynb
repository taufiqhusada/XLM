{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FAISS library was not found.\n",
      "FAISS not available. Switching to standard nearest neighbors search implementation.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "from xlm.utils import AttrDict\n",
    "from xlm.data.dictionary import Dictionary, BOS_WORD, EOS_WORD, PAD_WORD, UNK_WORD, MASK_WORD\n",
    "from xlm.model.transformer import TransformerModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reload a pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Supported languages: en, id\n"
     ]
    }
   ],
   "source": [
    "model_path = \"/projectnb/statnlp/gkuwanto/XLM/dumped/baseline_para_0/q3v4i6kl9t/best-valid_mlm_ppl.pth\"\n",
    "reloaded = torch.load(model_path)\n",
    "params = AttrDict(reloaded['params'])\n",
    "print(\"Supported languages: %s\" % \", \".join(params.lang2id.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build dictionary / update parameters / build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build dictionary / update parameters\n",
    "dico = Dictionary(reloaded['dico_id2word'], reloaded['dico_word2id'], reloaded['dico_counts'])\n",
    "params.n_words = len(dico)\n",
    "params.bos_index = dico.index(BOS_WORD)\n",
    "params.eos_index = dico.index(EOS_WORD)\n",
    "params.pad_index = dico.index(PAD_WORD)\n",
    "params.unk_index = dico.index(UNK_WORD)\n",
    "params.mask_index = dico.index(MASK_WORD)\n",
    "\n",
    "# build model / reload weights\n",
    "model = TransformerModel(params, dico, True, True)\n",
    "model.eval()\n",
    "\n",
    "from collections import OrderedDict\n",
    "reloaded_model = OrderedDict()\n",
    "for k, v in reloaded['model'].items():\n",
    "      reloaded_model[k.replace('module.', '')] = v\n",
    "model.load_state_dict(reloaded_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Get sentence representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentences have to be in the BPE format, i.e. tokenized sentences on which you applied fastBPE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below is one way to bpe-ize sentences\n",
    "codes = \"\" # path to the codes of the model\n",
    "fastbpe = os.path.join(os.getcwd(), 'tools/fastBPE/fast')\n",
    "\n",
    "def to_bpe(sentences):\n",
    "    # write sentences to tmp file\n",
    "    with open('/tmp/sentences.bpe', 'w') as fwrite:\n",
    "        for sent in sentences:\n",
    "            fwrite.write(sent + '\\n')\n",
    "    \n",
    "    # apply bpe to tmp file\n",
    "    os.system('%s applybpe /tmp/sentences.bpe /tmp/sentences %s' % (fastbpe, codes))\n",
    "    \n",
    "    # load bpe-ized sentences\n",
    "    sentences_bpe = []\n",
    "    with open('/tmp/sentences.bpe') as f:\n",
    "        for line in f:\n",
    "            sentences_bpe.append(line.rstrip())\n",
    "    \n",
    "    return sentences_bpe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warung ini dimiliki oleh pengusaha pabrik tahu yang sudah puluhan tahun terkenal membuat tahu putih di bandung . tahu berkualitas , dipadu keahlian memasak , dipadu kretivitas , jadilah warung yang menyajikan menu utama berbahan tahu , ditambah menu umum lain seperti ayam . semuanya selera indonesia . harga cukup terjangkau . jangan lewatkan tahu bletoka nya , tidak kalah dengan yang asli dari tegal !\n",
      "\n",
      "aaa\n",
      "Number of out-of-vocab words: 2/67\n"
     ]
    }
   ],
   "source": [
    "# Below are already BPE-ized sentences\n",
    "\n",
    "# list of (sentences, lang)\n",
    "sentences = [\n",
    "     'warung ini dimiliki oleh pengusaha pabrik tahu yang sudah puluhan tahun terkenal membuat tahu putih di bandung . tahu berkualitas , dipadu keahlian memasak , dipadu kretivitas , jadilah warung yang menyajikan menu utama berbahan tahu , ditambah menu umum lain seperti ayam . semuanya selera indonesia . harga cukup terjangkau . jangan lewatkan tahu bletoka nya , tidak kalah dengan yang asli dari tegal !',\n",
    "    'aaa'\n",
    "]\n",
    "\n",
    "# bpe-ize sentences\n",
    "sentences = to_bpe(sentences)\n",
    "print('\\n\\n'.join(sentences))\n",
    "\n",
    "# check how many tokens are OOV\n",
    "n_w = len([w for w in ' '.join(sentences).split()])\n",
    "n_oov = len([w for w in ' '.join(sentences).split() if w not in dico.word2id])\n",
    "print('Number of out-of-vocab words: %s/%s' % (n_oov, n_w))\n",
    "\n",
    "# add </s> sentence delimiters\n",
    "sentences = [(('</s> %s </s>' % sent.strip()).split()) for sent in sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = len(sentences)\n",
    "slen = max([len(sent) for sent in sentences])\n",
    "\n",
    "word_ids = torch.LongTensor(slen, bs).fill_(params.pad_index)\n",
    "for i in range(len(sentences)):\n",
    "    sent = torch.LongTensor([dico.index(w) for w in sentences[i]])\n",
    "    word_ids[:len(sent), i] = sent\n",
    "\n",
    "lengths = torch.LongTensor([len(sent) for sent in sentences])\n",
    "                             \n",
    "# NOTE: No more language id (removed it in a later version)\n",
    "langs = torch.LongTensor([params.lang2id['id']]).unsqueeze(0).expand(slen, bs) if params.n_langs > 1 else None\n",
    "# langs = torch.LongTensor([params.lang2id['id']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    1,     1],\n",
       "        [ 3367, 14369],\n",
       "        [   20,     1],\n",
       "        [ 1017,     2],\n",
       "        [   45,     2],\n",
       "        [ 1891,     2],\n",
       "        [ 1616,     2],\n",
       "        [  177,     2],\n",
       "        [   16,     2],\n",
       "        [   48,     2],\n",
       "        [ 2375,     2],\n",
       "        [   60,     2],\n",
       "        [  772,     2],\n",
       "        [   82,     2],\n",
       "        [  177,     2],\n",
       "        [  539,     2],\n",
       "        [   18,     2],\n",
       "        [  647,     2],\n",
       "        [   14,     2],\n",
       "        [  177,     2],\n",
       "        [ 1089,     2],\n",
       "        [   15,     2],\n",
       "        [25377,     2],\n",
       "        [ 3301,     2],\n",
       "        [ 2851,     2],\n",
       "        [   15,     2],\n",
       "        [25377,     2],\n",
       "        [    3,     2],\n",
       "        [   15,     2],\n",
       "        [ 4504,     2],\n",
       "        [ 3367,     2],\n",
       "        [   16,     2],\n",
       "        [ 3075,     2],\n",
       "        [  969,     2],\n",
       "        [  323,     2],\n",
       "        [ 4369,     2],\n",
       "        [  177,     2],\n",
       "        [   15,     2],\n",
       "        [ 1702,     2],\n",
       "        [  969,     2],\n",
       "        [  303,     2],\n",
       "        [   76,     2],\n",
       "        [   57,     2],\n",
       "        [  690,     2],\n",
       "        [   14,     2],\n",
       "        [  633,     2],\n",
       "        [ 3229,     2],\n",
       "        [   70,     2],\n",
       "        [   14,     2],\n",
       "        [   97,     2],\n",
       "        [  142,     2],\n",
       "        [ 1856,     2],\n",
       "        [   14,     2],\n",
       "        [  195,     2],\n",
       "        [ 9391,     2],\n",
       "        [  177,     2],\n",
       "        [    3,     2],\n",
       "        [  251,     2],\n",
       "        [   15,     2],\n",
       "        [   23,     2],\n",
       "        [ 1076,     2],\n",
       "        [   19,     2],\n",
       "        [   16,     2],\n",
       "        [  698,     2],\n",
       "        [   22,     2],\n",
       "        [ 6348,     2],\n",
       "        [   74,     2],\n",
       "        [    1,     2]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([68, 2, 1024])\n"
     ]
    }
   ],
   "source": [
    "tensor = model('fwd', x=word_ids, lengths=lengths, langs=langs, causal=False).contiguous()\n",
    "print(tensor.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "proj = nn.Sequential(*[\n",
    "    nn.Dropout(params.dropout),\n",
    "    nn.Linear(1024, 3)\n",
    "]).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = proj(tensor[0].cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1], device='cuda:0')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.data.max(1)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable `tensor` is of shape `(sequence_length, batch_size, model_dimension)`.\n",
    "\n",
    "`tensor[0]` is a tensor of shape `(batch_size, model_dimension)` that corresponds to the first hidden state of the last layer of each sentence.\n",
    "\n",
    "This is this vector that we use to finetune on the GLUE and XNLI tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# finetuning smsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append('../../gkuwanto/indonlu/')\n",
    "os.chdir('../../gkuwanto/indonlu/')\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "# from transformers import BertForSequenceClassification, BertConfig, BertTokenizer\n",
    "# from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "from utils.forward_fn import forward_sequence_classification\n",
    "from utils.metrics import document_sentiment_metrics_fn\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from utils.metrics import document_sentiment_metrics_fn\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "set_seed(33)\n",
    "\n",
    "\n",
    "class DocumentSentimentDataset(Dataset):\n",
    "    # Static constant variable\n",
    "    LABEL2INDEX = {'positive': 0, 'neutral': 1, 'negative': 2}\n",
    "    INDEX2LABEL = {0: 'positive', 1: 'neutral', 2: 'negative'}\n",
    "    NUM_LABELS = 3\n",
    "    \n",
    "    def load_dataset(self, path): \n",
    "        df = pd.read_csv(path, sep='\\t', header=None)\n",
    "        df.columns = ['text','sentiment']\n",
    "        df['sentiment'] = df['sentiment'].apply(lambda lab: self.LABEL2INDEX[lab])\n",
    "        return df\n",
    "    \n",
    "    def __init__(self, dataset_path, no_special_token=False, *args, **kwargs):\n",
    "        self.data = self.load_dataset(dataset_path)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        data = self.data.loc[index,:]\n",
    "        text, sentiment = data['text'], data['sentiment']\n",
    "        subwords = torch.LongTensor([dico.index(w) for w in text])\n",
    "      \n",
    "        return subwords, sentiment, text\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)    \n",
    "        \n",
    "class DocumentSentimentDataLoader(DataLoader):\n",
    "    def __init__(self, max_seq_len=512, *args, **kwargs):\n",
    "        super(DocumentSentimentDataLoader, self).__init__(*args, **kwargs)\n",
    "        self.collate_fn = self._collate_fn\n",
    "        self.max_seq_len = max_seq_len\n",
    "        \n",
    "    def _collate_fn(self, batch):\n",
    "        batch_size = len(batch)\n",
    "        max_seq_len = max(map(lambda x: len(x[0]), batch))\n",
    "        max_seq_len = min(self.max_seq_len, max_seq_len)\n",
    "        \n",
    "#         subword_batch = np.zeros((batch_size, max_seq_len), dtype=np.int64)\n",
    "        sentiment_batch = np.zeros((batch_size, 1), dtype=np.int64)\n",
    "        \n",
    "        seq_list = []\n",
    "        lengths = []\n",
    "\n",
    "        word_ids = torch.LongTensor(max_seq_len, batch_size).fill_(params.pad_index)\n",
    "        for i, (subwords, sentiment, raw_seq) in enumerate(batch):\n",
    "            subwords = subwords[:max_seq_len]\n",
    "            word_ids[:len(subwords), i] = subwords\n",
    "            sentiment_batch[i,0] = sentiment\n",
    "            seq_list.append(raw_seq)\n",
    "            \n",
    "            lengths.append(len(subwords))\n",
    "            \n",
    "            \n",
    "        lengths = torch.LongTensor(lengths)\n",
    "        \n",
    "        langs = torch.LongTensor([params.lang2id['id']]).unsqueeze(0).expand(max_seq_len, batch_size) if params.n_langs > 1 else None\n",
    "         \n",
    "#         print(word_ids)\n",
    "        return word_ids, sentiment_batch, lengths, langs, seq_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_path = './dataset/smsa_doc-sentiment-prosa/train_preprocess.tsv'\n",
    "valid_dataset_path = './dataset/smsa_doc-sentiment-prosa/valid_preprocess.tsv'\n",
    "test_dataset_path = './dataset/smsa_doc-sentiment-prosa/test_preprocess_masked_label.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = DocumentSentimentDataset(train_dataset_path, lowercase=True)\n",
    "valid_dataset = DocumentSentimentDataset(valid_dataset_path, lowercase=True)\n",
    "test_dataset = DocumentSentimentDataset(test_dataset_path, lowercase=True)\n",
    "\n",
    "train_loader = DocumentSentimentDataLoader(dataset=train_dataset, max_seq_len=512, batch_size=8, num_workers=16, shuffle=True)  \n",
    "valid_loader = DocumentSentimentDataLoader(dataset=valid_dataset, max_seq_len=512, batch_size=8, num_workers=16, shuffle=False)  \n",
    "test_loader = DocumentSentimentDataLoader(dataset=test_dataset, max_seq_len=512, batch_size=8, num_workers=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1669,  252, 1099, 1318,  677,  792,    3,  367,  677,  367,    3,  425,\n",
       "          367,  543,  367,  958,  367,  868,  367,    3,  749,  958,  655,  877,\n",
       "            3,  824,  655,  677,  792, 1318,  357,  252,  877,  252,    3,  824,\n",
       "          252,  458, 1099,  367,  868,    3,  710,  252,  877, 1318,    3,  882,\n",
       "          252,  677,  792,    3,  357, 1318,  425,  252,  877,    3,  824, 1318,\n",
       "          958, 1318,  877,  252,  677,    3,  710,  252,  877, 1318,  677,    3,\n",
       "          710,  655, 1099,  868,  655,  677,  252,  958,    3,  543,  655,  543,\n",
       "          458, 1318,  252,  710,    3,  710,  252,  877, 1318,    3,  824, 1318,\n",
       "          710,  367,  877,    3,  425,  367,    3,  458,  252,  677,  425, 1318,\n",
       "          677,  792,    3,   14,    3,  710,  252,  877, 1318,    3,  458,  655,\n",
       "         1099,  868, 1318,  252,  958,  367,  710,  252,  357,    3,   15,    3,\n",
       "          425,  367,  824,  252,  425, 1318,    3,  868,  655,  252,  877,  958,\n",
       "          367,  252,  677,    3,  543,  655,  543,  252,  357,  252,  868,    3,\n",
       "           15,    3,  425,  367,  824,  252,  425, 1318,    3,  868, 1099,  655,\n",
       "          710,  367, 1161,  367,  710,  252,  357,    3,   15,    3, 1665,  252,\n",
       "          425,  367,  958,  252,  877,    3, 1669,  252, 1099, 1318,  677,  792,\n",
       "            3,  882,  252,  677,  792,    3,  543,  655,  677,  882,  252, 1665,\n",
       "          367,  868,  252,  677,    3,  543,  655,  677, 1318,    3, 1318,  710,\n",
       "          252,  543,  252,    3,  458,  655, 1099,  458,  252,  877,  252,  677,\n",
       "            3,  710,  252,  877, 1318,    3,   15,    3,  425,  367,  710,  252,\n",
       "          543,  458,  252,  877,    3,  543,  655,  677, 1318,    3, 1318,  543,\n",
       "         1318,  543,    3,  958,  252,  367,  677,    3,  357,  655,  824,  655,\n",
       "         1099,  710,  367,    3,  252,  882,  252,  543,    3,   14,    3,  357,\n",
       "          655,  543, 1318,  252,  677,  882,  252,    3,  357,  655,  958,  655,\n",
       "         1099,  252,    3,  367,  677,  425,  749,  677,  655,  357,  367,  252,\n",
       "            3,   14,    3,  877,  252, 1099,  792,  252,    3,  524, 1318,  868,\n",
       "         1318,  824,    3,  710,  655, 1099, 1665,  252,  677,  792,  868,  252,\n",
       "         1318,    3,   14,    3, 1665,  252,  677,  792,  252,  677,    3,  958,\n",
       "          655, 1669,  252,  710,  868,  252,  677,    3,  710,  252,  877, 1318,\n",
       "            3,  458,  958,  655,  710,  749,  868,  252,    3,  677,  882,  252,\n",
       "            3,   15,    3,  710,  367,  425,  252,  868,    3,  868,  252,  958,\n",
       "          252,  877,    3,  425,  655,  677,  792,  252,  677,    3,  882,  252,\n",
       "          677,  792,    3,  252,  357,  958,  367,    3,  425,  252, 1099,  367,\n",
       "            3,  710,  655,  792,  252,  958,    3,   74]),\n",
       " 0,\n",
       " 'warung ini dimiliki oleh pengusaha pabrik tahu yang sudah puluhan tahun terkenal membuat tahu putih di bandung . tahu berkualitas , dipadu keahlian memasak , dipadu kretivitas , jadilah warung yang menyajikan menu utama berbahan tahu , ditambah menu umum lain seperti ayam . semuanya selera indonesia . harga cukup terjangkau . jangan lewatkan tahu bletoka nya , tidak kalah dengan yang asli dari tegal !')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'positive': 0, 'neutral': 1, 'negative': 2}\n",
      "{0: 'positive', 1: 'neutral', 2: 'negative'}\n"
     ]
    }
   ],
   "source": [
    "w2i, i2w = DocumentSentimentDataset.LABEL2INDEX, DocumentSentimentDataset.INDEX2LABEL\n",
    "print(w2i)\n",
    "print(i2w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_sequence_classification(proj, model, batch_data, i2w, is_test=False, device='cpu', **kwargs):\n",
    "    (subword_batch, label_batch, lengths, langs) = batch_data\n",
    "    token_type_batch = None\n",
    "    \n",
    "    # Prepare input & label\n",
    "    subword_batch = torch.LongTensor(subword_batch)\n",
    "    token_type_batch = torch.LongTensor(token_type_batch) if token_type_batch is not None else None\n",
    "    label_batch = torch.LongTensor(label_batch)\n",
    "            \n",
    "    if device == \"cuda\":\n",
    "        subword_batch = subword_batch.cuda()\n",
    "        token_type_batch = token_type_batch.cuda() if token_type_batch is not None else None\n",
    "        label_batch = label_batch.cuda()\n",
    "\n",
    "    # Forward model\n",
    "    logits = proj(model('fwd', x=subword_batch, lengths=lengths.cuda(), langs=langs, causal=False).contiguous()[0])\n",
    "#     print(logits)\n",
    "#     print(logits.shape)\n",
    "    \n",
    "#     print(logits)\n",
    "\n",
    "    \n",
    "#     loss, logits = outputs[:2]\n",
    "    \n",
    "#     # generate prediction & label list\n",
    "    list_hyp = []\n",
    "    list_label = []\n",
    "    hyp = torch.topk(logits, 1)[1]\n",
    "    for j in range(len(hyp)):\n",
    "        list_hyp.append(i2w[hyp[j].item()])\n",
    "        list_label.append(i2w[label_batch[j][0].item()])\n",
    "        \n",
    "#     return loss, list_hyp, list_label\n",
    "#     print(logits.data.max(1)[1])\n",
    "    loss = F.cross_entropy(logits, label_batch.squeeze(1))\n",
    "#     print(loss)\n",
    "\n",
    "    \n",
    "    return loss, logits, list_hyp, list_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_m = optim.Adam(model.parameters(), lr=1e-6)\n",
    "model = model.cuda()\n",
    "optimizer_p = optim.Adam(proj.parameters(), lr=1e-6)\n",
    "proj = proj.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 1) TRAIN LOSS:0.7781: 100%|██████████| 1375/1375 [06:59<00:00,  3.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 1) TRAIN LOSS:0.7781 {'ACC': 0.6574545454545454, 'F1': 0.4547214168108535, 'REC': 0.46195095492013927, 'PRE': 0.5081770697870992}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.7449: 100%|██████████| 158/158 [00:14<00:00, 10.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 1) VALID LOSS:0.7449 {'ACC': 0.676984126984127, 'F1': 0.4772894253441345, 'REC': 0.48508958045185757, 'PRE': 0.5565250821871808}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 2) TRAIN LOSS:0.7148: 100%|██████████| 1375/1375 [07:00<00:00,  3.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 2) TRAIN LOSS:0.7148 {'ACC': 0.6838181818181818, 'F1': 0.5150837418194335, 'REC': 0.508131377539731, 'PRE': 0.5732957690178956}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.7159: 100%|██████████| 158/158 [00:14<00:00, 10.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 2) VALID LOSS:0.7159 {'ACC': 0.6936507936507936, 'F1': 0.551934418722865, 'REC': 0.5356415629115727, 'PRE': 0.6030639573242383}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 3) TRAIN LOSS:0.6849: 100%|██████████| 1375/1375 [06:58<00:00,  3.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 3) TRAIN LOSS:0.6849 {'ACC': 0.7029090909090909, 'F1': 0.5701187432957601, 'REC': 0.5539168936288078, 'PRE': 0.6147266711747602}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.6999: 100%|██████████| 158/158 [00:14<00:00, 10.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 3) VALID LOSS:0.6999 {'ACC': 0.703968253968254, 'F1': 0.5417616929312836, 'REC': 0.5310963811520139, 'PRE': 0.6505847036316535}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 3\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    proj.train()\n",
    "    torch.set_grad_enabled(True)\n",
    "    \n",
    "    total_train_loss = 0\n",
    "\n",
    "    list_hyp, list_label = [], []\n",
    "    \n",
    "    train_pbar = tqdm(train_loader, leave=True, total=len(train_loader))\n",
    "    for i, batch_data in enumerate(train_pbar):\n",
    "        # Forward model\n",
    "        loss, logits, batch_hyp, batch_label = forward_sequence_classification(proj, model, batch_data[:-1], i2w=i2w, device='cuda')\n",
    "#         print(loss)\n",
    "        \n",
    "        optimizer_m.zero_grad()\n",
    "        optimizer_p.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer_m.step()\n",
    "        optimizer_p.step()\n",
    "        \n",
    "        tr_loss = loss.item()\n",
    "        total_train_loss = total_train_loss + tr_loss\n",
    "        \n",
    "        list_hyp += batch_hyp\n",
    "        list_label += batch_label\n",
    "        \n",
    "        train_pbar.set_description(\"(Epoch {}) TRAIN LOSS:{:.4f}\".format((epoch+1),\n",
    "            total_train_loss/(i+1)))\n",
    "        \n",
    "        \n",
    "    # Calculate train metric\n",
    "    metrics = document_sentiment_metrics_fn(list_hyp, list_label)\n",
    "    print(\"(Epoch {}) TRAIN LOSS:{:.4f} {}\".format((epoch+1),\n",
    "        total_train_loss/(i+1),metrics))\n",
    "    \n",
    "    \n",
    "        \n",
    "        \n",
    "    # Evaluate on validation\n",
    "    model.eval()\n",
    "    proj.eval()\n",
    "    torch.set_grad_enabled(False)\n",
    "    \n",
    "    total_loss, total_correct, total_labels = 0, 0, 0\n",
    "    \n",
    "    list_hyp, list_label = [], []\n",
    "\n",
    "    pbar = tqdm(valid_loader, leave=True, total=len(valid_loader))\n",
    "    for i, batch_data in enumerate(pbar):\n",
    "        batch_seq = batch_data[-1]        \n",
    "        loss, logits, batch_hyp, batch_label = forward_sequence_classification(proj, model, batch_data[:-1], i2w=i2w, device='cuda')\n",
    "        \n",
    "        # Calculate total loss\n",
    "        valid_loss = loss.item()\n",
    "        total_loss = total_loss + valid_loss\n",
    "\n",
    "        # Calculate evaluation metrics\n",
    "        list_hyp += batch_hyp\n",
    "        list_label += batch_label\n",
    "        \n",
    "        pbar.set_description(\"VALID LOSS:{:.4f}\".format(total_loss/(i+1)))\n",
    "        \n",
    "    metrics = document_sentiment_metrics_fn(list_hyp, list_label)\n",
    "    print(\"(Epoch {}) VALID LOSS:{:.4f} {}\".format((epoch+1),\n",
    "        total_loss/(i+1), metrics))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:03<00:00, 16.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     index     label\n",
      "0        0  positive\n",
      "1        1  positive\n",
      "2        2  positive\n",
      "3        3  positive\n",
      "4        4  positive\n",
      "..     ...       ...\n",
      "495    495  negative\n",
      "496    496  negative\n",
      "497    497  negative\n",
      "498    498  negative\n",
      "499    499  negative\n",
      "\n",
      "[500 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Evaluate on test\n",
    "model.eval()\n",
    "proj.eval()\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "total_loss, total_correct, total_labels = 0, 0, 0\n",
    "list_hyp, list_label = [], []\n",
    "\n",
    "pbar = tqdm(test_loader, leave=True, total=len(test_loader))\n",
    "for i, batch_data in enumerate(pbar):\n",
    "    loss, logits, batch_hyp, batch_label = forward_sequence_classification(proj, model, batch_data[:-1], i2w=i2w, device='cuda')\n",
    "    list_hyp += batch_hyp\n",
    "\n",
    "# Save prediction\n",
    "df = pd.DataFrame({'label':list_hyp}).reset_index()\n",
    "# df.to_csv('pred.txt', index=False)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "negative    260\n",
       "positive    236\n",
       "neutral       4\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('/projectnb/statnlp/gik/XLM/output/pred-smsa.tsv', index=False, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), '/projectnb/statnlp/gik/XLM/output/smsa_xlm_finetuned_model.pth')\n",
    "torch.save(proj.state_dict(), '/projectnb/statnlp/gik/XLM/output/smsa_proj.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
