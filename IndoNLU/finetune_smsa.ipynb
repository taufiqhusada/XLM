{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FAISS library was not found.\n",
      "FAISS not available. Switching to standard nearest neighbors search implementation.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "os.chdir('../')\n",
    "\n",
    "\n",
    "from xlm.utils import AttrDict\n",
    "from xlm.data.dictionary import Dictionary, BOS_WORD, EOS_WORD, PAD_WORD, UNK_WORD, MASK_WORD\n",
    "from xlm.model.transformer import TransformerModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reload a pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Supported languages: en, id\n"
     ]
    }
   ],
   "source": [
    "model_path = \"/projectnb/statnlp/gkuwanto/XLM/dumped/baseline_para_0/q3v4i6kl9t/best-valid_mlm_ppl.pth\"\n",
    "reloaded = torch.load(model_path)\n",
    "params = AttrDict(reloaded['params'])\n",
    "print(\"Supported languages: %s\" % \", \".join(params.lang2id.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build dictionary / update parameters / build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build dictionary / update parameters\n",
    "dico = Dictionary(reloaded['dico_id2word'], reloaded['dico_word2id'], reloaded['dico_counts'])\n",
    "params.n_words = len(dico)\n",
    "params.bos_index = dico.index(BOS_WORD)\n",
    "params.eos_index = dico.index(EOS_WORD)\n",
    "params.pad_index = dico.index(PAD_WORD)\n",
    "params.unk_index = dico.index(UNK_WORD)\n",
    "params.mask_index = dico.index(MASK_WORD)\n",
    "\n",
    "# build model / reload weights\n",
    "model = TransformerModel(params, dico, True, True)\n",
    "model.eval()\n",
    "\n",
    "from collections import OrderedDict\n",
    "reloaded_model = OrderedDict()\n",
    "for k, v in reloaded['model'].items():\n",
    "      reloaded_model[k.replace('module.', '')] = v\n",
    "model.load_state_dict(reloaded_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Get sentence representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentences have to be in the BPE format, i.e. tokenized sentences on which you applied fastBPE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below is one way to bpe-ize sentences\n",
    "codes = \"\" # path to the codes of the model\n",
    "fastbpe = os.path.join(os.getcwd(), 'tools/fastBPE/fast')\n",
    "\n",
    "def to_bpe(sentences):\n",
    "    # write sentences to tmp file\n",
    "    with open('/tmp/sentences.bpe', 'w') as fwrite:\n",
    "        for sent in sentences:\n",
    "            fwrite.write(sent + '\\n')\n",
    "    \n",
    "    # apply bpe to tmp file\n",
    "    os.system('%s applybpe /tmp/sentences.bpe /tmp/sentences %s' % (fastbpe, codes))\n",
    "    \n",
    "    # load bpe-ized sentences\n",
    "    sentences_bpe = []\n",
    "    with open('/tmp/sentences.bpe') as f:\n",
    "        for line in f:\n",
    "            sentences_bpe.append(line.rstrip())\n",
    "    \n",
    "    return sentences_bpe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warung ini dimiliki oleh pengusaha pabrik tahu yang sudah puluhan tahun terkenal membuat tahu putih di bandung . tahu berkualitas , dipadu keahlian memasak , dipadu kretivitas , jadilah warung yang menyajikan menu utama berbahan tahu , ditambah menu umum lain seperti ayam . semuanya selera indonesia . harga cukup terjangkau . jangan lewatkan tahu bletoka nya , tidak kalah dengan yang asli dari tegal !\n",
      "\n",
      "aaa\n",
      "Number of out-of-vocab words: 2/67\n"
     ]
    }
   ],
   "source": [
    "# Below are already BPE-ized sentences\n",
    "\n",
    "# list of (sentences, lang)\n",
    "sentences = [\n",
    "     'warung ini dimiliki oleh pengusaha pabrik tahu yang sudah puluhan tahun terkenal membuat tahu putih di bandung . tahu berkualitas , dipadu keahlian memasak , dipadu kretivitas , jadilah warung yang menyajikan menu utama berbahan tahu , ditambah menu umum lain seperti ayam . semuanya selera indonesia . harga cukup terjangkau . jangan lewatkan tahu bletoka nya , tidak kalah dengan yang asli dari tegal !',\n",
    "    'aaa'\n",
    "]\n",
    "\n",
    "# bpe-ize sentences\n",
    "sentences = to_bpe(sentences)\n",
    "print('\\n\\n'.join(sentences))\n",
    "\n",
    "# check how many tokens are OOV\n",
    "n_w = len([w for w in ' '.join(sentences).split()])\n",
    "n_oov = len([w for w in ' '.join(sentences).split() if w not in dico.word2id])\n",
    "print('Number of out-of-vocab words: %s/%s' % (n_oov, n_w))\n",
    "\n",
    "# add </s> sentence delimiters\n",
    "sentences = [(('</s> %s </s>' % sent.strip()).split()) for sent in sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = len(sentences)\n",
    "slen = max([len(sent) for sent in sentences])\n",
    "\n",
    "word_ids = torch.LongTensor(slen, bs).fill_(params.pad_index)\n",
    "for i in range(len(sentences)):\n",
    "    sent = torch.LongTensor([dico.index(w) for w in sentences[i]])\n",
    "    word_ids[:len(sent), i] = sent\n",
    "\n",
    "lengths = torch.LongTensor([len(sent) for sent in sentences])\n",
    "                             \n",
    "# NOTE: No more language id (removed it in a later version)\n",
    "langs = torch.LongTensor([params.lang2id['id']]).unsqueeze(0).expand(slen, bs) if params.n_langs > 1 else None\n",
    "# langs = torch.LongTensor([params.lang2id['id']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    1,     1],\n",
       "        [ 3367, 14369],\n",
       "        [   20,     1],\n",
       "        [ 1017,     2],\n",
       "        [   45,     2],\n",
       "        [ 1891,     2],\n",
       "        [ 1616,     2],\n",
       "        [  177,     2],\n",
       "        [   16,     2],\n",
       "        [   48,     2],\n",
       "        [ 2375,     2],\n",
       "        [   60,     2],\n",
       "        [  772,     2],\n",
       "        [   82,     2],\n",
       "        [  177,     2],\n",
       "        [  539,     2],\n",
       "        [   18,     2],\n",
       "        [  647,     2],\n",
       "        [   14,     2],\n",
       "        [  177,     2],\n",
       "        [ 1089,     2],\n",
       "        [   15,     2],\n",
       "        [25377,     2],\n",
       "        [ 3301,     2],\n",
       "        [ 2851,     2],\n",
       "        [   15,     2],\n",
       "        [25377,     2],\n",
       "        [    3,     2],\n",
       "        [   15,     2],\n",
       "        [ 4504,     2],\n",
       "        [ 3367,     2],\n",
       "        [   16,     2],\n",
       "        [ 3075,     2],\n",
       "        [  969,     2],\n",
       "        [  323,     2],\n",
       "        [ 4369,     2],\n",
       "        [  177,     2],\n",
       "        [   15,     2],\n",
       "        [ 1702,     2],\n",
       "        [  969,     2],\n",
       "        [  303,     2],\n",
       "        [   76,     2],\n",
       "        [   57,     2],\n",
       "        [  690,     2],\n",
       "        [   14,     2],\n",
       "        [  633,     2],\n",
       "        [ 3229,     2],\n",
       "        [   70,     2],\n",
       "        [   14,     2],\n",
       "        [   97,     2],\n",
       "        [  142,     2],\n",
       "        [ 1856,     2],\n",
       "        [   14,     2],\n",
       "        [  195,     2],\n",
       "        [ 9391,     2],\n",
       "        [  177,     2],\n",
       "        [    3,     2],\n",
       "        [  251,     2],\n",
       "        [   15,     2],\n",
       "        [   23,     2],\n",
       "        [ 1076,     2],\n",
       "        [   19,     2],\n",
       "        [   16,     2],\n",
       "        [  698,     2],\n",
       "        [   22,     2],\n",
       "        [ 6348,     2],\n",
       "        [   74,     2],\n",
       "        [    1,     2]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024\n"
     ]
    }
   ],
   "source": [
    "tensor = model('fwd', x=word_ids, lengths=lengths, langs=langs, causal=False).contiguous()\n",
    "print(tensor.size()[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "proj = nn.Sequential(*[\n",
    "    nn.Dropout(params.dropout),\n",
    "    nn.Linear(1024, 3)\n",
    "]).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = proj(tensor[0].cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2], device='cuda:0')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.data.max(1)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable `tensor` is of shape `(sequence_length, batch_size, model_dimension)`.\n",
    "\n",
    "`tensor[0]` is a tensor of shape `(batch_size, model_dimension)` that corresponds to the first hidden state of the last layer of each sentence.\n",
    "\n",
    "This is this vector that we use to finetune on the GLUE and XNLI tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# finetuning smsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xlm_indo_nlu_utils.data_loader_utils import DocumentSentimentDataset, DocumentSentimentDataLoader\n",
    "\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "from utils.forward_fn import forward_sequence_classification\n",
    "from utils.metrics import document_sentiment_metrics_fn\n",
    "\n",
    "\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "set_seed(33333)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_path = './dataset/smsa_doc-sentiment-prosa/train_preprocess.tsv'\n",
    "valid_dataset_path = './dataset/smsa_doc-sentiment-prosa/valid_preprocess.tsv'\n",
    "test_dataset_path = './dataset/smsa_doc-sentiment-prosa/test_preprocess_masked_label.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = DocumentSentimentDataset(train_dataset_path, dico, params, lowercase=True)\n",
    "valid_dataset = DocumentSentimentDataset(valid_dataset_path, dico, params, lowercase=True)\n",
    "test_dataset = DocumentSentimentDataset(test_dataset_path,dico, params, lowercase=True)\n",
    "\n",
    "train_loader = DocumentSentimentDataLoader(dataset=train_dataset, params=params, max_seq_len=512, batch_size=16, num_workers=16, shuffle=True)  \n",
    "valid_loader = DocumentSentimentDataLoader(dataset=valid_dataset, params=params, max_seq_len=512,  batch_size=16, num_workers=16, shuffle=False)  \n",
    "test_loader = DocumentSentimentDataLoader(dataset=test_dataset, params=params, max_seq_len=512, batch_size=16, num_workers=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1669,  252, 1099, 1318,  677,  792,    3,  367,  677,  367,    3,  425,\n",
       "          367,  543,  367,  958,  367,  868,  367,    3,  749,  958,  655,  877,\n",
       "            3,  824,  655,  677,  792, 1318,  357,  252,  877,  252,    3,  824,\n",
       "          252,  458, 1099,  367,  868,    3,  710,  252,  877, 1318,    3,  882,\n",
       "          252,  677,  792,    3,  357, 1318,  425,  252,  877,    3,  824, 1318,\n",
       "          958, 1318,  877,  252,  677,    3,  710,  252,  877, 1318,  677,    3,\n",
       "          710,  655, 1099,  868,  655,  677,  252,  958,    3,  543,  655,  543,\n",
       "          458, 1318,  252,  710,    3,  710,  252,  877, 1318,    3,  824, 1318,\n",
       "          710,  367,  877,    3,  425,  367,    3,  458,  252,  677,  425, 1318,\n",
       "          677,  792,    3,   14,    3,  710,  252,  877, 1318,    3,  458,  655,\n",
       "         1099,  868, 1318,  252,  958,  367,  710,  252,  357,    3,   15,    3,\n",
       "          425,  367,  824,  252,  425, 1318,    3,  868,  655,  252,  877,  958,\n",
       "          367,  252,  677,    3,  543,  655,  543,  252,  357,  252,  868,    3,\n",
       "           15,    3,  425,  367,  824,  252,  425, 1318,    3,  868, 1099,  655,\n",
       "          710,  367, 1161,  367,  710,  252,  357,    3,   15,    3, 1665,  252,\n",
       "          425,  367,  958,  252,  877,    3, 1669,  252, 1099, 1318,  677,  792,\n",
       "            3,  882,  252,  677,  792,    3,  543,  655,  677,  882,  252, 1665,\n",
       "          367,  868,  252,  677,    3,  543,  655,  677, 1318,    3, 1318,  710,\n",
       "          252,  543,  252,    3,  458,  655, 1099,  458,  252,  877,  252,  677,\n",
       "            3,  710,  252,  877, 1318,    3,   15,    3,  425,  367,  710,  252,\n",
       "          543,  458,  252,  877,    3,  543,  655,  677, 1318,    3, 1318,  543,\n",
       "         1318,  543,    3,  958,  252,  367,  677,    3,  357,  655,  824,  655,\n",
       "         1099,  710,  367,    3,  252,  882,  252,  543,    3,   14,    3,  357,\n",
       "          655,  543, 1318,  252,  677,  882,  252,    3,  357,  655,  958,  655,\n",
       "         1099,  252,    3,  367,  677,  425,  749,  677,  655,  357,  367,  252,\n",
       "            3,   14,    3,  877,  252, 1099,  792,  252,    3,  524, 1318,  868,\n",
       "         1318,  824,    3,  710,  655, 1099, 1665,  252,  677,  792,  868,  252,\n",
       "         1318,    3,   14,    3, 1665,  252,  677,  792,  252,  677,    3,  958,\n",
       "          655, 1669,  252,  710,  868,  252,  677,    3,  710,  252,  877, 1318,\n",
       "            3,  458,  958,  655,  710,  749,  868,  252,    3,  677,  882,  252,\n",
       "            3,   15,    3,  710,  367,  425,  252,  868,    3,  868,  252,  958,\n",
       "          252,  877,    3,  425,  655,  677,  792,  252,  677,    3,  882,  252,\n",
       "          677,  792,    3,  252,  357,  958,  367,    3,  425,  252, 1099,  367,\n",
       "            3,  710,  655,  792,  252,  958,    3,   74]),\n",
       " 0,\n",
       " 'warung ini dimiliki oleh pengusaha pabrik tahu yang sudah puluhan tahun terkenal membuat tahu putih di bandung . tahu berkualitas , dipadu keahlian memasak , dipadu kretivitas , jadilah warung yang menyajikan menu utama berbahan tahu , ditambah menu umum lain seperti ayam . semuanya selera indonesia . harga cukup terjangkau . jangan lewatkan tahu bletoka nya , tidak kalah dengan yang asli dari tegal !')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'positive': 0, 'neutral': 1, 'negative': 2}\n",
      "{0: 'positive', 1: 'neutral', 2: 'negative'}\n"
     ]
    }
   ],
   "source": [
    "w2i, i2w = DocumentSentimentDataset.LABEL2INDEX, DocumentSentimentDataset.INDEX2LABEL\n",
    "print(w2i)\n",
    "print(i2w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xlm_indo_nlu_utils.model_utils import forward_sequence_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_m = optim.Adam(model.parameters(), lr=3e-5)\n",
    "model = model.cuda()\n",
    "optimizer_p = optim.Adam(proj.parameters(), lr=3e-5)\n",
    "proj = proj.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 1) TRAIN LOSS:0.8439:   8%|▊         | 57/688 [00:18<03:19,  3.16it/s]Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x2ae710311f70>\n",
      "Traceback (most recent call last):\n",
      "  File \"/projectnb/statnlp/thdaryan/.conda/envs/xlm/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/projectnb/statnlp/thdaryan/.conda/envs/xlm/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1301, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/projectnb/statnlp/thdaryan/.conda/envs/xlm/lib/python3.9/multiprocessing/process.py\", line 149, in join\n",
      "    res = self._popen.wait(timeout)\n",
      "  File \"/projectnb/statnlp/thdaryan/.conda/envs/xlm/lib/python3.9/multiprocessing/popen_fork.py\", line 40, in wait\n",
      "    if not wait([self.sentinel], timeout):\n",
      "  File \"/projectnb/statnlp/thdaryan/.conda/envs/xlm/lib/python3.9/multiprocessing/connection.py\", line 936, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/projectnb/statnlp/thdaryan/.conda/envs/xlm/lib/python3.9/selectors.py\", line 416, in select\n",
      "    fd_event_list = self._selector.poll(timeout)\n",
      "KeyboardInterrupt: \n",
      "(Epoch 1) TRAIN LOSS:0.8439:   8%|▊         | 57/688 [00:19<03:38,  2.88it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-b1d13366923e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0moptimizer_m\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0moptimizer_p\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0moptimizer_m\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0moptimizer_p\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/projectnb/statnlp/thdaryan/.conda/envs/xlm/lib/python3.9/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/projectnb/statnlp/thdaryan/.conda/envs/xlm/lib/python3.9/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_epochs = 5\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    proj.train()\n",
    "    torch.set_grad_enabled(True)\n",
    "    \n",
    "    total_train_loss = 0\n",
    "\n",
    "    list_hyp, list_label = [], []\n",
    "    \n",
    "    train_pbar = tqdm(train_loader, leave=True, total=len(train_loader))\n",
    "    for i, batch_data in enumerate(train_pbar):\n",
    "        # Forward model\n",
    "        loss, logits, batch_hyp, batch_label = forward_sequence_classification(proj, model, batch_data[:-1], i2w=i2w, device='cuda')\n",
    "#         print(loss)\n",
    "        \n",
    "        optimizer_m.zero_grad()\n",
    "        optimizer_p.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer_m.step()\n",
    "        optimizer_p.step()\n",
    "        \n",
    "        tr_loss = loss.item()\n",
    "        total_train_loss = total_train_loss + tr_loss\n",
    "        \n",
    "        list_hyp += batch_hyp\n",
    "        list_label += batch_label\n",
    "        \n",
    "        train_pbar.set_description(\"(Epoch {}) TRAIN LOSS:{:.4f}\".format((epoch+1),\n",
    "            total_train_loss/(i+1)))\n",
    "        \n",
    "        \n",
    "    # Calculate train metric\n",
    "    metrics = document_sentiment_metrics_fn(list_hyp, list_label)\n",
    "    print(\"(Epoch {}) TRAIN LOSS:{:.4f} {}\".format((epoch+1),\n",
    "        total_train_loss/(i+1),metrics))\n",
    "    \n",
    "    \n",
    "        \n",
    "        \n",
    "    # Evaluate on validation\n",
    "    model.eval()\n",
    "    proj.eval()\n",
    "    torch.set_grad_enabled(False)\n",
    "    \n",
    "    total_loss, total_correct, total_labels = 0, 0, 0\n",
    "    \n",
    "    list_hyp, list_label = [], []\n",
    "\n",
    "    pbar = tqdm(valid_loader, leave=True, total=len(valid_loader))\n",
    "    for i, batch_data in enumerate(pbar):\n",
    "        batch_seq = batch_data[-1]        \n",
    "        loss, logits, batch_hyp, batch_label = forward_sequence_classification(proj, model, batch_data[:-1], i2w=i2w, device='cuda')\n",
    "        \n",
    "        # Calculate total loss\n",
    "        valid_loss = loss.item()\n",
    "        total_loss = total_loss + valid_loss\n",
    "\n",
    "        # Calculate evaluation metrics\n",
    "        list_hyp += batch_hyp\n",
    "        list_label += batch_label\n",
    "        \n",
    "        pbar.set_description(\"VALID LOSS:{:.4f}\".format(total_loss/(i+1)))\n",
    "        \n",
    "    metrics = document_sentiment_metrics_fn(list_hyp, list_label)\n",
    "    print(\"(Epoch {}) VALID LOSS:{:.4f} {}\".format((epoch+1),\n",
    "        total_loss/(i+1), metrics))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Evaluate on test\n",
    "model.eval()\n",
    "proj.eval()\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "total_loss, total_correct, total_labels = 0, 0, 0\n",
    "list_hyp, list_label = [], []\n",
    "\n",
    "pbar = tqdm(test_loader, leave=True, total=len(test_loader))\n",
    "for i, batch_data in enumerate(pbar):\n",
    "    loss, logits, batch_hyp, batch_label = forward_sequence_classification(proj, model, batch_data[:-1], i2w=i2w, device='cuda')\n",
    "    list_hyp += batch_hyp\n",
    "\n",
    "# Save prediction\n",
    "df = pd.DataFrame({'label':list_hyp}).reset_index()\n",
    "# df.to_csv('pred.txt', index=False)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv('/projectnb/statnlp/gik/XLM/output/pred-smsa.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), '/projectnb/statnlp/gik/XLM/output/smsa_xlm_finetuned_model.pth')\n",
    "# torch.save(proj.state_dict(), '/projectnb/statnlp/gik/XLM/output/smsa_proj.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
